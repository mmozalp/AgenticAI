{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "635d8ebb",
      "metadata": {},
      "source": [
        "# Academic QA System with GraphRAG\n",
        "\n",
        "- Author: [Yongdam Kim](https://github.com/dancing-with-coffee)\n",
        "- Design: \n",
        "- Peer Review: \n",
        "- This is a part of [LangChain Open Tutorial](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial)\n",
        "\n",
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/99-TEMPLATE/00-BASE-TEMPLATE-EXAMPLE.ipynb) [![Open in GitHub](https://img.shields.io/badge/Open%20in%20GitHub-181717?style=flat-square&logo=github&logoColor=white)](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/99-TEMPLATE/00-BASE-TEMPLATE-EXAMPLE.ipynb)\n",
        "\n",
        "## Overview\n",
        "\n",
        "This tutorial demonstrates how to implement a QA system that better leverages paper (academic) content by using GraphRAG.\n",
        "\n",
        "GraphRAG is a novel system introduced by Microsoft that utilizes a graph to extract both local and global information from text, providing more contextually rich answers.\n",
        "\n",
        "However, Microsoft’s official GraphRAG implementation is not readily integrated with LangChain, making it difficult to use.\n",
        "\n",
        "To solve this, we use `langchain-graphrag` which allows us to implement GraphRAG within LangChain.\n",
        "\n",
        "In this tutorial, we’ll learn how to build a QA system for the latest AI papers using `langchain-graphrag`.\n",
        "\n",
        "![GraphRAG](./assets/08-academicqasystem-graphrag-pipeline.png)\n",
        "\n",
        "> from arXiv- \"From Local to Global-A Graph RAG Approach to Query-Focused Summarization\"\n",
        "\n",
        "### Table of Contents\n",
        "\n",
        "- [Overview](#overview)\n",
        "- [Environment Setup](#environment-setup)\n",
        "- [Download and Load arXiv PDFs](#download-and-load-arxiv-pdfs)\n",
        "- [Text Chunking and Text Extracting](#text-chunking-and-text-extracting)\n",
        "- [Graph Generation](#graph-generation)\n",
        "- [Graph Index Build](#graph-index-build)\n",
        "- [Local Search through Knowledge Graph](#local-search-through-knowledge-graph)\n",
        "- [Global Search through Knowledge Graph](#global-search-through-knowledge-graph)\n",
        "\n",
        "\n",
        "\n",
        "### References\n",
        "\n",
        "1. [langchain-graphrag github repo](https://github.com/ksachdeva/langchain-graphrag)\n",
        "2. [GraphRAG paper](https://arxiv.org/pdf/2404.16130)\n",
        "3. [GraphRAG Microsoft Official Research Blog post](https://www.microsoft.com/en-us/research/blog/graphrag-unlocking-llm-discovery-on-narrative-private-data/)\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6c7aba4",
      "metadata": {},
      "source": [
        "## Environment Setup\n",
        "\n",
        "Set up the environment. You may refer to [Environment Setup](https://wikidocs.net/257836) for more details.\n",
        "\n",
        "**[Note]**\n",
        "- `langchain-opentutorial` is a package that provides a set of easy-to-use environment setup, useful functions and utilities for tutorials.\n",
        "- You can check out the [`langchain-opentutorial`](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "21943adb",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "!pip install langchain-opentutorial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "f25ec196",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages.\n",
        "# 'langchain_opentutorial.package.install' is a helper function that installs\n",
        "# the specified packages inside this environment.\n",
        "\n",
        "from langchain_opentutorial import package\n",
        "\n",
        "package.install(\n",
        "    [\n",
        "        \"langsmith\",\n",
        "        \"langchain\",\n",
        "        \"langchain_core\",\n",
        "        \"langchain_community\",\n",
        "        \"langchain-graphrag\",\n",
        "        \"langchain_chroma\",\n",
        "        \"jq\",\n",
        "    ],\n",
        "    verbose=False,\n",
        "    upgrade=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "690a9ae0",
      "metadata": {},
      "source": [
        "You can alternatively set API keys such as `OPENAI_API_KEY` in a `.env` file and load them.\n",
        "\n",
        "[Note] This is not necessary if you've already set the required API keys in previous steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "4f99b5b6",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load environment variables from a .env file (e.g. OPENAI_API_KEY)\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv(override=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa00c3f4",
      "metadata": {},
      "source": [
        "## Download and Load arXiv PDFs\n",
        "\n",
        "In this tutorial, we will use arXiv data. arXiv is an online archive for the latest research papers, all available in PDF format. There is an official GitHub repository containing all PDFs, but it is about 1TB in total size and can only be downloaded from AWS.\n",
        "Thus, in this tutorial, we will selectively use a few PDF files instead.\n",
        "\n",
        "- Link to the full dataset: https://github.com/mattbierbaum/arxiv-public-datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "69cb77da",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PDF downloaded and saved to: ./data/2404.16130v1.pdf\n",
            "Loaded 15 documents.\n",
            "From Local to Global: A Graph RAG Approach to\n",
            "Query-Focused Summarization\n",
            "Darren Edge1†Ha Trinh1†Newman Cheng2Joshua Bradley2Alex Chao3\n",
            "Apurva Mody3Steven Truitt2\n",
            "Jonathan Larson1\n",
            "1Microsoft Research\n",
            "2Microsoft Strategic Missions and Technologies\n",
            "3Microsoft Office of the CTO\n",
            "{daedge,trinhha,newmancheng,joshbradley,achao,moapurva,steventruitt,jolarso }\n",
            "@microsoft.com\n",
            "†These authors contributed equally to this work\n",
            "Abstract\n",
            "The use of retrieval-augmented generation (RAG) to retrieve relevant informa-\n",
            "tion from an external knowledge source enables large language models (LLMs)\n",
            "to answer questions over private and/or previously unseen document collections.\n",
            "However, RAG fails on global questions directed at an entire text corpus, such\n",
            "as “What are the main themes in the dataset?”, since this is inherently a query-\n",
            "focused summarization (QFS) task, rather than an explicit retrieval task. Prior\n",
            "QFS methods, meanwhile, fail to scale to the quantities of text indexed by typical\n",
            "RAG systems. To combine the strengths of these contrasting methods, we propose\n",
            "a Graph RAG approach to question answering over private text corpora that scales\n",
            "with both the generality of user questions and the quantity of source text to be in-\n",
            "dexed. Our approach uses an LLM to build a graph-based text index in two stages:\n",
            "first to derive an entity knowledge graph from the source documents, then to pre-\n",
            "generate community summaries for all groups of closely-related entities. Given a\n",
            "question, each community summary is used to generate a partial response, before\n",
            "all partial responses are again summarized in a final response to the user. For a\n",
            "class of global sensemaking questions over datasets in the 1 million token range,\n",
            "we show that Graph RAG leads to substantial improvements over a na ¨ıve RAG\n",
            "baseline for both the comprehensiveness and diversity of generated answers. An\n",
            "open-source, Python-based implementation of both global and local Graph RAG\n",
            "approaches is forthcoming at https://aka .ms/graphrag .\n",
            "1 Introduction\n",
            "Human endeavors across a range of domains rely on our ability to read and reason about large\n",
            "collections of documents, often reaching conclusions that go beyond anything stated in the source\n",
            "texts themselves. With the emergence of large language models (LLMs), we are already witnessing\n",
            "attempts to automate human-like sensemaking in complex domains like scientific discovery (Mi-\n",
            "crosoft, 2023) and intelligence analysis (Ranade and Joshi, 2023), where sensemaking is defined as\n",
            "Preprint. Under review.arXiv:2404.16130v1  [cs.CL]  24 Apr 2024\n"
          ]
        }
      ],
      "source": [
        "# Download and save sample PDF file to ./data directory\n",
        "import requests\n",
        "import os\n",
        "\n",
        "def download_pdf(url, save_path):\n",
        "    \"\"\"\n",
        "    Downloads a PDF file from the given URL and saves it to the specified path.\n",
        "\n",
        "    Args:\n",
        "        url (str): The URL of the PDF file to download.\n",
        "        save_path (str): The full path (including file name) where the file will be saved.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Ensure the directory exists\n",
        "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "\n",
        "        # Download the file\n",
        "        response = requests.get(url, stream=True)\n",
        "        response.raise_for_status()  # Raise an error for bad status codes\n",
        "\n",
        "        # Save the file to the specified path\n",
        "        with open(save_path, \"wb\") as file:\n",
        "            for chunk in response.iter_content(chunk_size=8192):\n",
        "                file.write(chunk)\n",
        "\n",
        "        print(f\"PDF downloaded and saved to: {save_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while downloading the file: {e}\")\n",
        "\n",
        "# Configuration for the PDF file\n",
        "pdf_url = \"https://arxiv.org/pdf/2404.16130v1\"\n",
        "file_path = \"./data/2404.16130v1.pdf\"\n",
        "\n",
        "# Download the PDF\n",
        "download_pdf(pdf_url, file_path)\n",
        "\n",
        "# Load the GraphRAG paper using PyPDFLoader.\n",
        "# PyPDFLoader loads PDF content on a per-page basis.\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(file_path)\n",
        "docs = loader.load()\n",
        "print(f\"Loaded {len(docs)} documents.\")\n",
        "print(docs[0].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b2fc536",
      "metadata": {},
      "source": [
        "## Text Chunking and Text Extracting\n",
        "\n",
        "In this step, we perform **query routing** and **document evaluation**. These steps are crucial parts of **Adaptive RAG**, contributing to efficient information retrieval and generation.\n",
        "\n",
        "- **Query Routing**: Analyzes the user’s query to route it to the appropriate information source.\n",
        "- **Document Evaluation**: Evaluates the quality and relevance of the retrieved documents.\n",
        "\n",
        "These steps support the core functions of **Adaptive RAG**, aiming to provide accurate and reliable information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "1b78d33f",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting text units ...: 100%|██████████| 6/6 [00:00<00:00, 10437.92it/s]\n",
            "Extracting text units ...: 100%|██████████| 8/8 [00:00<00:00, 15148.73it/s]\n",
            "Extracting text units ...: 100%|██████████| 8/8 [00:00<00:00, 23746.94it/s]\n",
            "Extracting text units ...: 100%|██████████| 10/10 [00:00<00:00, 113359.57it/s]\n",
            "Extracting text units ...: 100%|██████████| 7/7 [00:00<00:00, 23413.18it/s]\n",
            "Extracting text units ...: 100%|██████████| 8/8 [00:00<00:00, 43976.98it/s]\n",
            "Extracting text units ...: 100%|██████████| 9/9 [00:00<00:00, 46603.38it/s]\n",
            "Extracting text units ...: 100%|██████████| 11/11 [00:00<00:00, 43815.14it/s]\n",
            "Extracting text units ...: 100%|██████████| 8/8 [00:00<00:00, 11188.54it/s]\n",
            "Extracting text units ...: 100%|██████████| 10/10 [00:00<00:00, 149263.49it/s]\n",
            "Extracting text units ...: 100%|██████████| 10/10 [00:00<00:00, 79891.50it/s]\n",
            "Extracting text units ...: 100%|██████████| 8/8 [00:00<00:00, 22429.43it/s]\n",
            "Extracting text units ...: 100%|██████████| 9/9 [00:00<00:00, 21794.88it/s]\n",
            "Extracting text units ...: 100%|██████████| 8/8 [00:00<00:00, 97541.95it/s]\n",
            "Extracting text units ...: 100%|██████████| 2/2 [00:00<00:00, 39199.10it/s]\n",
            "Processing documents ...: 100%|██████████| 15/15 [00:00<00:00, 506.48it/s]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>document_id</th>\n",
              "      <th>id</th>\n",
              "      <th>text_unit</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1e3e4efb-27de-44c1-9854-4b4176ed618b</td>\n",
              "      <td>6fe80d15-f2b2-47f2-8570-b5a64042d2f5</td>\n",
              "      <td>From Local to Global: A Graph RAG Approach to\\...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1e3e4efb-27de-44c1-9854-4b4176ed618b</td>\n",
              "      <td>7a73bcf9-3874-40eb-ad70-e6c1070dd207</td>\n",
              "      <td>tion from an external knowledge source enables...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1e3e4efb-27de-44c1-9854-4b4176ed618b</td>\n",
              "      <td>63e41d22-ac60-48b8-8e70-d3e88ea2a7bc</td>\n",
              "      <td>RAG systems. To combine the strengths of these...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1e3e4efb-27de-44c1-9854-4b4176ed618b</td>\n",
              "      <td>861b4797-2cfd-4811-9cdf-efe0eb6a2bee</td>\n",
              "      <td>question, each community summary is used to ge...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1e3e4efb-27de-44c1-9854-4b4176ed618b</td>\n",
              "      <td>d543cef6-4565-4b73-9364-075b325ae290</td>\n",
              "      <td>approaches is forthcoming at https://aka .ms/g...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117</th>\n",
              "      <td>65133966-6472-4c24-8520-40fccbfd666b</td>\n",
              "      <td>52cd8901-0e27-4f0e-93e2-bfeade1cdad9</td>\n",
              "      <td>with chain-of-thought reasoning for knowledge-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>118</th>\n",
              "      <td>65133966-6472-4c24-8520-40fccbfd666b</td>\n",
              "      <td>36449b6c-150a-451a-b6fc-1a3081757068</td>\n",
              "      <td>Wang, Y ., Lipka, N., Rossi, R. A., Siu, A., Z...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119</th>\n",
              "      <td>65133966-6472-4c24-8520-40fccbfd666b</td>\n",
              "      <td>7f1c0194-ba84-4fe5-b81f-659aa62d89cf</td>\n",
              "      <td>Empirical Methods in Natural Language Processi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120</th>\n",
              "      <td>c4225aec-3b51-4bd6-bb1b-53c66d42229c</td>\n",
              "      <td>e73e8ca0-417d-462c-8c84-bce611bf50e4</td>\n",
              "      <td>Yao, L., Peng, J., Mao, C., and Luo, Y . (2023...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>121</th>\n",
              "      <td>c4225aec-3b51-4bd6-bb1b-53c66d42229c</td>\n",
              "      <td>94b7803f-07dc-45a3-a12f-cec090a7f721</td>\n",
              "      <td>Zheng, L., Chiang, W.-L., Sheng, Y ., Zhuang, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>122 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                              document_id  \\\n",
              "0    1e3e4efb-27de-44c1-9854-4b4176ed618b   \n",
              "1    1e3e4efb-27de-44c1-9854-4b4176ed618b   \n",
              "2    1e3e4efb-27de-44c1-9854-4b4176ed618b   \n",
              "3    1e3e4efb-27de-44c1-9854-4b4176ed618b   \n",
              "4    1e3e4efb-27de-44c1-9854-4b4176ed618b   \n",
              "..                                    ...   \n",
              "117  65133966-6472-4c24-8520-40fccbfd666b   \n",
              "118  65133966-6472-4c24-8520-40fccbfd666b   \n",
              "119  65133966-6472-4c24-8520-40fccbfd666b   \n",
              "120  c4225aec-3b51-4bd6-bb1b-53c66d42229c   \n",
              "121  c4225aec-3b51-4bd6-bb1b-53c66d42229c   \n",
              "\n",
              "                                       id  \\\n",
              "0    6fe80d15-f2b2-47f2-8570-b5a64042d2f5   \n",
              "1    7a73bcf9-3874-40eb-ad70-e6c1070dd207   \n",
              "2    63e41d22-ac60-48b8-8e70-d3e88ea2a7bc   \n",
              "3    861b4797-2cfd-4811-9cdf-efe0eb6a2bee   \n",
              "4    d543cef6-4565-4b73-9364-075b325ae290   \n",
              "..                                    ...   \n",
              "117  52cd8901-0e27-4f0e-93e2-bfeade1cdad9   \n",
              "118  36449b6c-150a-451a-b6fc-1a3081757068   \n",
              "119  7f1c0194-ba84-4fe5-b81f-659aa62d89cf   \n",
              "120  e73e8ca0-417d-462c-8c84-bce611bf50e4   \n",
              "121  94b7803f-07dc-45a3-a12f-cec090a7f721   \n",
              "\n",
              "                                             text_unit  \n",
              "0    From Local to Global: A Graph RAG Approach to\\...  \n",
              "1    tion from an external knowledge source enables...  \n",
              "2    RAG systems. To combine the strengths of these...  \n",
              "3    question, each community summary is used to ge...  \n",
              "4    approaches is forthcoming at https://aka .ms/g...  \n",
              "..                                                 ...  \n",
              "117  with chain-of-thought reasoning for knowledge-...  \n",
              "118  Wang, Y ., Lipka, N., Rossi, R. A., Siu, A., Z...  \n",
              "119  Empirical Methods in Natural Language Processi...  \n",
              "120  Yao, L., Peng, J., Mao, C., and Luo, Y . (2023...  \n",
              "121  Zheng, L., Chiang, W.-L., Sheng, Y ., Zhuang, ...  \n",
              "\n",
              "[122 rows x 3 columns]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Here we split the loaded documents into text chunks.\n",
        "# 'RecursiveCharacterTextSplitter' is a commonly used utility in LangChain for\n",
        "# chunking text with some overlap.\n",
        "\n",
        "from langchain_core.documents import Document\n",
        "from langchain_graphrag.indexing import TextUnitExtractor\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=64)\n",
        "text_unit_extractor = TextUnitExtractor(text_splitter=splitter)\n",
        "\n",
        "# This runs the text splitting logic on the loaded PDF pages\n",
        "df_text_units = text_unit_extractor.run(docs)\n",
        "df_text_units"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ef1bd80",
      "metadata": {},
      "source": [
        "## Entity Relationship Extraction\n",
        "\n",
        "GraphRAG extracts entities and relationships from the text chunks to automatically build a knowledge graph.\n",
        "\n",
        "When constructing a Knowledge Graph, an LLM is used. In this tutorial, we use `gpt-4o-mini` for performance and cost reasons. The LLM uses a predefined prompt to extract entity and relationship information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "ea34acb0",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting entities and relationships ...: 100%|██████████| 122/122 [12:53<00:00,  6.34s/it]\n"
          ]
        }
      ],
      "source": [
        "# This process can take about 20 minutes.\n",
        "# EntityRelationshipExtractor uses a language model to identify entities & relationships in the text.\n",
        "\n",
        "from langchain_graphrag.indexing.graph_generation import EntityRelationshipExtractor\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# We instantiate a ChatOpenAI with 'gpt-4o-mini' model.\n",
        "er_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)\n",
        "\n",
        "# Build the default entity-relationship extractor.\n",
        "extractor = EntityRelationshipExtractor.build_default(llm=er_llm)\n",
        "\n",
        "# Invoke extractor on the text units (chunks)\n",
        "text_unit_graphs = extractor.invoke(df_text_units)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "6f5b50ed",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "Graph: 0\n",
            "Number of nodes - 11\n",
            "Number of edges - 8\n",
            "['DARREN EDGE', 'HA TRINH', 'NEWMAN CHENG', 'JOSHUA BRADLEY', 'ALEX CHAO', 'APURVA MODY', 'STEVEN TRUITT', 'JONATHAN LARSON', 'MICROSOFT RESEARCH', 'MICROSOFT STRATEGIC MISSIONS AND TECHNOLOGIES', 'MICROSOFT OFFICE OF THE CTO']\n",
            "[('DARREN EDGE', 'MICROSOFT RESEARCH'), ('HA TRINH', 'MICROSOFT RESEARCH'), ('NEWMAN CHENG', 'MICROSOFT STRATEGIC MISSIONS AND TECHNOLOGIES'), ('JOSHUA BRADLEY', 'MICROSOFT STRATEGIC MISSIONS AND TECHNOLOGIES'), ('ALEX CHAO', 'MICROSOFT OFFICE OF THE CTO'), ('APURVA MODY', 'MICROSOFT OFFICE OF THE CTO'), ('STEVEN TRUITT', 'MICROSOFT STRATEGIC MISSIONS AND TECHNOLOGIES'), ('JONATHAN LARSON', 'MICROSOFT RESEARCH')]\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "Graph: 1\n",
            "Number of nodes - 3\n",
            "Number of edges - 2\n",
            "['LARGE LANGUAGE MODELS', 'RAG', 'QFS']\n",
            "[('LARGE LANGUAGE MODELS', 'RAG'), ('RAG', 'QFS')]\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "Graph: 2\n",
            "Number of nodes - 3\n",
            "Number of edges - 2\n",
            "['RAG SYSTEMS', 'GRAPH RAG', 'LLM']\n",
            "[('RAG SYSTEMS', 'GRAPH RAG'), ('GRAPH RAG', 'LLM')]\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "Graph: 3\n",
            "Number of nodes - 3\n",
            "Number of edges - 2\n",
            "['GRAPH RAG', 'PYTHON', 'USER']\n",
            "[('GRAPH RAG', 'USER'), ('GRAPH RAG', 'PYTHON')]\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "Graph: 4\n",
            "Number of nodes - 2\n",
            "Number of edges - 1\n",
            "['HUMAN ENDEAVORS', 'LARGE LANGUAGE MODELS']\n",
            "[('HUMAN ENDEAVORS', 'LARGE LANGUAGE MODELS')]\n",
            "---------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Display the graph information (nodes/edges) extracted from each chunk.\n",
        "for index, g in enumerate(text_unit_graphs):\n",
        "    if index == 5: # show 5 graphs\n",
        "        break\n",
        "    print(\"---------------------------------\")\n",
        "    print(f\"Graph: {index}\")\n",
        "    print(f\"Number of nodes - {len(g.nodes)}\")\n",
        "    print(f\"Number of edges - {len(g.edges)}\")\n",
        "    print(g.nodes())\n",
        "    print(g.edges())\n",
        "    print(\"---------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "745a33df",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'type': 'ORGANIZATION',\n",
              " 'description': ['Graph RAG is an approach that utilizes a graph-based text index to enhance question answering capabilities.'],\n",
              " 'text_unit_ids': ['63e41d22-ac60-48b8-8e70-d3e88ea2a7bc']}"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Example: search for a specific extracted node\n",
        "text_unit_graphs[2].nodes[\"GRAPH RAG\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "9b611c4d",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'weight': 1.0,\n",
              " 'description': ['LLM is used in the Graph RAG approach to build a graph-based text index'],\n",
              " 'text_unit_ids': ['63e41d22-ac60-48b8-8e70-d3e88ea2a7bc']}"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Example: check the relationship (edge) between two extracted entities\n",
        "text_unit_graphs[2].edges[(\"GRAPH RAG\", \"LLM\")]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f0fa221",
      "metadata": {},
      "source": [
        "## Graph Generation\n",
        "\n",
        "GraphRAG does not use all extracted entities and relationships individually; it merges them into a more comprehensive structure. We call this process Summarization.\n",
        "\n",
        "Through element summarization, GraphRAG enhances search functionality by improving global context understanding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "04a442d4",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting entities and relationships ...: 100%|██████████| 122/122 [13:00<00:00,  6.40s/it]\n",
            "Summarizing entities descriptions: 100%|██████████| 482/482 [02:40<00:00,  2.99it/s]\n",
            "Summarizing relationship descriptions: 100%|██████████| 689/689 [00:30<00:00, 22.51it/s]\n"
          ]
        }
      ],
      "source": [
        "# This process can take about 22 minutes.\n",
        "# We merge all local graphs into a single consolidated graph, then run summarization.\n",
        "\n",
        "from langchain_graphrag.indexing.graph_generation import (\n",
        "    GraphsMerger,\n",
        "    EntityRelationshipDescriptionSummarizer,\n",
        "    GraphGenerator,\n",
        ")\n",
        "\n",
        "graphs_merger = GraphsMerger()\n",
        "\n",
        "es_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)\n",
        "\n",
        "summarizer = EntityRelationshipDescriptionSummarizer.build_default(llm=es_llm)\n",
        "\n",
        "graph_generator = GraphGenerator(\n",
        "    er_extractor=extractor,\n",
        "    graphs_merger=GraphsMerger(),\n",
        "    er_description_summarizer=summarizer,\n",
        ")\n",
        "\n",
        "# Execute the graph generation.\n",
        "graph = graph_generator.run(df_text_units)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "08954af3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of nodes - 482\n",
            "Number of edges - 689\n"
          ]
        }
      ],
      "source": [
        "# Check how many nodes and edges are in the final merged+summarized graph.\n",
        "print(f\"Number of nodes - {len(graph[0].nodes)}\")\n",
        "print(f\"Number of edges - {len(graph[0].edges)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "711ccb9e",
      "metadata": {},
      "source": [
        "## Graph Index Build\n",
        "\n",
        "- We run all steps from Text Chunking to Community Detection and Community Summarization in code.\n",
        "- For community detection, we use the Leiden algorithm, known for good performance.\n",
        "- In GraphRAG, we create an index called an artifact. We ultimately store the artifact using the `save_artifact` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "4643f1c5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Below we define functions to save and load the final Graph Index artifacts.\n",
        "\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from langchain_graphrag.indexing.artifacts import IndexerArtifacts\n",
        "\n",
        "\n",
        "# This function saves the IndexerArtifacts object to disk.\n",
        "def save_artifacts(artifacts: IndexerArtifacts, path: str):\n",
        "    artifacts.entities.to_parquet(f\"{path}/entities.parquet\")\n",
        "    artifacts.relationships.to_parquet(f\"{path}/relationships.parquet\")\n",
        "    artifacts.text_units.to_parquet(f\"{path}/text_units.parquet\")\n",
        "    artifacts.communities_reports.to_parquet(f\"{path}/communities_reports.parquet\")\n",
        "\n",
        "    if artifacts.merged_graph is not None:\n",
        "        with path.joinpath(\"merged-graph.pickle\").open(\"wb\") as fp:\n",
        "            pickle.dump(artifacts.merged_graph, fp)\n",
        "\n",
        "    if artifacts.summarized_graph is not None:\n",
        "        with path.joinpath(\"summarized-graph.pickle\").open(\"wb\") as fp:\n",
        "            pickle.dump(artifacts.summarized_graph, fp)\n",
        "\n",
        "    if artifacts.communities is not None:\n",
        "        with path.joinpath(\"community_info.pickle\").open(\"wb\") as fp:\n",
        "            pickle.dump(artifacts.communities, fp)\n",
        "\n",
        "\n",
        "# This function loads the IndexerArtifacts object from disk.\n",
        "def load_artifacts(path: Path) -> IndexerArtifacts:\n",
        "    entities = pd.read_parquet(f\"{path}/entities.parquet\")\n",
        "    relationships = pd.read_parquet(f\"{path}/relationships.parquet\")\n",
        "    text_units = pd.read_parquet(f\"{path}/text_units.parquet\")\n",
        "    communities_reports = pd.read_parquet(f\"{path}/communities_reports.parquet\")\n",
        "\n",
        "    merged_graph = None\n",
        "    summarized_graph = None\n",
        "    communities = None\n",
        "\n",
        "    merged_graph_pickled = path.joinpath(\"merged-graph.pickle\")\n",
        "    if merged_graph_pickled.exists():\n",
        "        with merged_graph_pickled.open(\"rb\") as fp:\n",
        "            merged_graph = pickle.load(fp)\n",
        "\n",
        "    summarized_graph_pickled = path.joinpath(\"summarized-graph.pickle\")\n",
        "    if summarized_graph_pickled.exists():\n",
        "        with summarized_graph_pickled.open(\"rb\") as fp:\n",
        "            summarized_graph = pickle.load(fp)\n",
        "\n",
        "    community_info_pickled = path.joinpath(\"community_info.pickle\")\n",
        "    if community_info_pickled.exists():\n",
        "        with community_info_pickled.open(\"rb\") as fp:\n",
        "            communities = pickle.load(fp)\n",
        "\n",
        "    return IndexerArtifacts(\n",
        "        entities,\n",
        "        relationships,\n",
        "        text_units,\n",
        "        communities_reports,\n",
        "        merged_graph=merged_graph,\n",
        "        summarized_graph=summarized_graph,\n",
        "        communities=communities,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "18564a43",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Matplotlib is building the font cache; this may take a moment.\n",
            "Extracting text units ...: 100%|██████████| 6/6 [00:00<00:00, 30320.27it/s]\n",
            "Extracting text units ...: 100%|██████████| 8/8 [00:00<00:00, 21690.00it/s]\n",
            "Extracting text units ...: 100%|██████████| 8/8 [00:00<00:00, 26132.74it/s]\n",
            "Extracting text units ...: 100%|██████████| 10/10 [00:00<00:00, 32488.80it/s]\n",
            "Extracting text units ...: 100%|██████████| 7/7 [00:00<00:00, 111212.61it/s]\n",
            "Extracting text units ...: 100%|██████████| 8/8 [00:00<00:00, 59178.89it/s]\n",
            "Extracting text units ...: 100%|██████████| 9/9 [00:00<00:00, 182361.04it/s]\n",
            "Extracting text units ...: 100%|██████████| 11/11 [00:00<00:00, 40294.62it/s]\n",
            "Extracting text units ...: 100%|██████████| 8/8 [00:00<00:00, 29720.49it/s]\n",
            "Extracting text units ...: 100%|██████████| 10/10 [00:00<00:00, 54899.27it/s]\n",
            "Extracting text units ...: 100%|██████████| 10/10 [00:00<00:00, 140748.46it/s]\n",
            "Extracting text units ...: 100%|██████████| 8/8 [00:00<00:00, 68338.97it/s]\n",
            "Extracting text units ...: 100%|██████████| 9/9 [00:00<00:00, 79471.02it/s]\n",
            "Extracting text units ...: 100%|██████████| 8/8 [00:00<00:00, 111476.52it/s]\n",
            "Extracting text units ...: 100%|██████████| 2/2 [00:00<00:00, 62601.55it/s]\n",
            "Processing documents ...: 100%|██████████| 15/15 [00:00<00:00, 706.08it/s]\n",
            "Extracting entities and relationships ...: 100%|██████████| 122/122 [14:50<00:00,  7.30s/it]\n",
            "Summarizing entities descriptions: 100%|██████████| 489/489 [02:08<00:00,  3.80it/s]\n",
            "Summarizing relationship descriptions: 100%|██████████| 500/500 [00:26<00:00, 19.00it/s]\n",
            "Generating report for level=0 commnuity_id=9: 100%|██████████| 10/10 [01:43<00:00, 10.35s/it]\n",
            "Generating report for level=1 commnuity_id=30: 100%|██████████| 21/21 [03:16<00:00,  9.37s/it]\n",
            "Generating report for level=2 commnuity_id=32: 100%|██████████| 2/2 [00:17<00:00,  8.95s/it]\n"
          ]
        }
      ],
      "source": [
        "# The entire indexing pipeline: from chunking, graph generation, community detection to generating final artifacts (entities, relationships, communities, etc.).\n",
        "# This step can take about 22 minutes.\n",
        "\n",
        "from langchain_chroma.vectorstores import Chroma as ChromaVectorStore\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_graphrag.indexing import SimpleIndexer\n",
        "from langchain_graphrag.indexing.artifacts_generation import (\n",
        "    CommunitiesReportsArtifactsGenerator,\n",
        "    EntitiesArtifactsGenerator,\n",
        "    RelationshipsArtifactsGenerator,\n",
        "    TextUnitsArtifactsGenerator,\n",
        ")\n",
        "from langchain_graphrag.indexing.graph_clustering.leiden_community_detector import (\n",
        "    HierarchicalLeidenCommunityDetector,\n",
        ")\n",
        "from langchain_graphrag.indexing.report_generation import (\n",
        "    CommunityReportGenerator,\n",
        "    CommunityReportWriter,\n",
        ")\n",
        "\n",
        "# Initialize the community detector\n",
        "community_detector = HierarchicalLeidenCommunityDetector()\n",
        "\n",
        "# Define the LLM and embedding model\n",
        "ls_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "# Create a Chroma vector store for entities.\n",
        "entities_collection_name = f\"entity-openai-embeddings\"\n",
        "entities_vector_store = ChromaVectorStore(\n",
        "    collection_name=entities_collection_name,\n",
        "    persist_directory=\"./\",\n",
        "    embedding_function=embeddings,\n",
        ")\n",
        "\n",
        "# Generators for different artifact types\n",
        "entities_artifacts_generator = EntitiesArtifactsGenerator(\n",
        "    entities_vector_store=entities_vector_store\n",
        ")\n",
        "relationships_artifacts_generator = RelationshipsArtifactsGenerator()\n",
        "\n",
        "# Community Report Generator & Writer\n",
        "report_generator = CommunityReportGenerator.build_default(\n",
        "    llm=ls_llm,\n",
        "    chain_config={\"tags\": [\"community-report\"]},\n",
        ")\n",
        "report_writer = CommunityReportWriter()\n",
        "\n",
        "communities_report_artifacts_generator = CommunitiesReportsArtifactsGenerator(\n",
        "    report_generator=report_generator,\n",
        "    report_writer=report_writer,\n",
        ")\n",
        "\n",
        "# For text units\n",
        "text_units_artifacts_generator = TextUnitsArtifactsGenerator()\n",
        "\n",
        "# The SimpleIndexer orchestrates all indexing steps\n",
        "indexer = SimpleIndexer(\n",
        "    text_unit_extractor=text_unit_extractor,\n",
        "    graph_generator=graph_generator,\n",
        "    community_detector=community_detector,\n",
        "    entities_artifacts_generator=entities_artifacts_generator,\n",
        "    relationships_artifacts_generator=relationships_artifacts_generator,\n",
        "    text_units_artifacts_generator=text_units_artifacts_generator,\n",
        "    communities_report_artifacts_generator=communities_report_artifacts_generator,\n",
        ")\n",
        "\n",
        "# Run the entire pipeline on the loaded docs\n",
        "artifacts = indexer.run(docs)\n",
        "\n",
        "# Save the final artifacts to disk\n",
        "artifacts_dir = Path(\"./\")\n",
        "save_artifacts(artifacts, artifacts_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e846a24",
      "metadata": {},
      "source": [
        "## Local Search through Knowledge Graph\n",
        "\n",
        "We perform a local search using the Knowledge Graph built by GraphRAG. Local Search is helpful for retrieving specific passages or details. Compare this with a simple gpt-4o-mini answer. The GraphRAG-based answer is much more detailed and grounded in the paper content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "adc83f41",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the artifacts we saved earlier\n",
        "artifacts = load_artifacts(artifacts_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "e31fcc42",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Graph RAG utilizes the **Leiden algorithm** for community detection within its framework. The Leiden algorithm is recognized for its efficiency in identifying communities in large networks, making it a pivotal tool in the analysis of the MultiHop-RAG dataset, which is integral to the Graph RAG approach. This algorithm enhances the understanding of relationships among different entities by effectively partitioning graphs into well-connected communities [Data: Entities (88); Relationships (149, 160)].\n",
            "\n",
            "### Significance of the Leiden Algorithm\n",
            "\n",
            "The Leiden algorithm improves upon earlier methods, such as the Louvain method, by ensuring that the resulting communities reflect meaningful relationships among nodes. Its ability to recover hierarchical community structures is particularly valuable for analyzing large-scale graphs, which is a common challenge in network analysis. This capability is essential for the comprehensive data analysis that Graph RAG aims to achieve [Data: Entities (88); Reports (12)].\n",
            "\n",
            "### Application in Graph RAG\n",
            "\n",
            "In the context of Graph RAG, the Leiden algorithm is employed to analyze the MultiHop-RAG dataset, which encompasses a wide range of news articles across various categories. This application underscores the importance of computational methods in extracting meaningful insights from complex data structures, thereby enhancing the overall quality of information retrieval and summarization processes [Data: Reports (0); Relationships (149)].\n",
            "\n",
            "In summary, the Leiden algorithm plays a crucial role in the community detection capabilities of Graph RAG, facilitating effective data analysis and enhancing the understanding of complex relationships within large datasets.\n"
          ]
        }
      ],
      "source": [
        "# Now we demonstrate local search on the knowledge graph.\n",
        "\n",
        "from typing import cast\n",
        "\n",
        "from langchain_graphrag.query.local_search import (\n",
        "    LocalSearch,\n",
        "    LocalSearchPromptBuilder,\n",
        "    LocalSearchRetriever,\n",
        ")\n",
        "from langchain_graphrag.query.local_search.context_builders import (\n",
        "    ContextBuilder,\n",
        ")\n",
        "from langchain_graphrag.query.local_search.context_selectors import (\n",
        "    ContextSelector,\n",
        ")\n",
        "from langchain_graphrag.types.graphs.community import CommunityLevel\n",
        "from langchain_graphrag.utils import TiktokenCounter\n",
        "\n",
        "# Build a default ContextSelector that will figure out which entities and communities\n",
        "# are most relevant based on the user query.\n",
        "context_selector = ContextSelector.build_default(\n",
        "    entities_vector_store=entities_vector_store,\n",
        "    entities_top_k=10,\n",
        "    community_level=cast(CommunityLevel, 2),  # e.g. second-level community granularity\n",
        ")\n",
        "\n",
        "# The ContextBuilder merges the context from the selection step into a final prompt context.\n",
        "context_builder = ContextBuilder.build_default(\n",
        "    token_counter=TiktokenCounter(),\n",
        ")\n",
        "\n",
        "# LocalSearchRetriever uses the context_selector and context_builder to retrieve relevant data.\n",
        "retriever = LocalSearchRetriever(\n",
        "    context_selector=context_selector,\n",
        "    context_builder=context_builder,\n",
        "    artifacts=artifacts,\n",
        ")\n",
        "\n",
        "# LocalSearch ties everything together into a query chain.\n",
        "local_search = LocalSearch(\n",
        "    prompt_builder=LocalSearchPromptBuilder(),\n",
        "    llm=ls_llm,\n",
        "    retriever=retriever,\n",
        ")\n",
        "\n",
        "# We get a chain object by calling local_search().\n",
        "search_chain = local_search()\n",
        "\n",
        "# Let's make a query.\n",
        "query = \"What community detection algorithm does GraphRAG use?\"\n",
        "print(search_chain.invoke(query))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c6b2759",
      "metadata": {},
      "source": [
        "## Global Search through Knowledge Graph\n",
        "\n",
        "We can also perform a global search using the Knowledge Graph built by GraphRAG. A global search is useful for getting answers with broader context. However, global search requires a model with a sufficiently large max token length. For example, `gpt-4o-mini` (max token size = 16k) may not handle the entire content of a large paper-based graph. If possible, try `gpt-4o` (max token size = 32k) for larger contexts!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "04ebf119",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Reached max tokens for a community report call ...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "## Community Detection Algorithm in GraphRAG\n",
            "\n",
            "GraphRAG utilizes the **Leiden algorithm** for community detection. This algorithm is specifically designed to partition graphs into well-connected communities, which enhances the understanding of relationships among different entities within the dataset. The Leiden algorithm represents a significant advancement over earlier methodologies, such as the Louvain method, by providing improved performance and accuracy in detecting communities within complex networks [Data: Reports (8, 12)].\n",
            "\n",
            "### Implications of Using the Leiden Algorithm\n",
            "\n",
            "The choice of the Leiden algorithm indicates a focus on achieving more reliable and meaningful community structures. This may lead to better insights into the interactions and connections among various entities represented in the data. By employing this advanced algorithm, GraphRAG may facilitate more effective analysis and interpretation of the underlying relationships, which is crucial for applications that rely on community detection.\n",
            "\n",
            "In summary, the use of the Leiden algorithm in GraphRAG enhances its capability to analyze and interpret complex relationships, thereby providing a robust framework for community detection in the dataset.\n"
          ]
        }
      ],
      "source": [
        "# Demonstrate global search using the knowledge graph.\n",
        "# Here, we generate key points from the entire graph and then aggregate them.\n",
        "\n",
        "from langchain_graphrag.query.global_search import GlobalSearch\n",
        "from langchain_graphrag.query.global_search.community_weight_calculator import (\n",
        "    CommunityWeightCalculator,\n",
        ")\n",
        "from langchain_graphrag.query.global_search.key_points_aggregator import (\n",
        "    KeyPointsAggregator,\n",
        "    KeyPointsAggregatorPromptBuilder,\n",
        "    KeyPointsContextBuilder,\n",
        ")\n",
        "from langchain_graphrag.query.global_search.key_points_generator import (\n",
        "    CommunityReportContextBuilder,\n",
        "    KeyPointsGenerator,\n",
        "    KeyPointsGeneratorPromptBuilder,\n",
        ")\n",
        "\n",
        "# The CommunityReportContextBuilder creates a context from community reports based on user query.\n",
        "report_context_builder = CommunityReportContextBuilder(\n",
        "    community_level=cast(CommunityLevel, 1),\n",
        "    weight_calculator=CommunityWeightCalculator(),\n",
        "    artifacts=artifacts,\n",
        "    token_counter=TiktokenCounter(),\n",
        "    max_tokens=16384,  # This is the limit for 'gpt-4o-mini'.\n",
        ")\n",
        "\n",
        "# KeyPointsGenerator creates key points from the available content.\n",
        "kp_generator = KeyPointsGenerator(\n",
        "    llm=ls_llm,\n",
        "    prompt_builder=KeyPointsGeneratorPromptBuilder(\n",
        "        show_references=True, repeat_instructions=True\n",
        "    ),\n",
        "    context_builder=report_context_builder,\n",
        ")\n",
        "\n",
        "# KeyPointsAggregator merges (aggregates) the key points from each relevant community.\n",
        "kp_aggregator = KeyPointsAggregator(\n",
        "    llm=ls_llm,\n",
        "    prompt_builder=KeyPointsAggregatorPromptBuilder(\n",
        "        show_references=True, repeat_instructions=True\n",
        "    ),\n",
        "    context_builder=KeyPointsContextBuilder(token_counter=TiktokenCounter()),\n",
        "    output_raw=False,\n",
        ")\n",
        "\n",
        "# GlobalSearch orchestrates the generation and aggregation of key points for an entire knowledge graph.\n",
        "global_search = GlobalSearch(\n",
        "    kp_generator=kp_generator,\n",
        "    kp_aggregator=kp_aggregator,\n",
        "    generation_chain_config={\"tags\": [\"kp-generation\"]},\n",
        "    aggregation_chain_config={\"tags\": [\"kp-aggregation\"]},\n",
        ")\n",
        "\n",
        "# Perform a synchronous invoke of the global search.\n",
        "response = global_search.invoke(query)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffdfca51",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "langchain-opentutorial-4P_5uzwg-py3.11",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
