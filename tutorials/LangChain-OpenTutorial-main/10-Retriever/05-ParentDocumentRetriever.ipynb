{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "635d8ebb",
      "metadata": {
        "id": "635d8ebb"
      },
      "source": [
        "# Parent Document Retriever\n",
        "\n",
        "- Author: [Yun Eun](https://github.com/yuneun92)\n",
        "- Design:\n",
        "- Peer Review:\n",
        "- This is a part of [LangChain Open Tutorial](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial)\n",
        "\n",
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/99-TEMPLATE/00-BASE-TEMPLATE-EXAMPLE.ipynb) [![Open in GitHub](https://img.shields.io/badge/Open%20in%20GitHub-181717?style=flat-square&logo=github&logoColor=white)](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/99-TEMPLATE/00-BASE-TEMPLATE-EXAMPLE.ipynb)\n",
        "\n",
        "## Overview\n",
        "\n",
        "This tutorial focuses on the `ParentDocumentRetriever` implementation, a tool designed to balance document search and chunking.\n",
        "\n",
        "When splitting documents for search, two competing needs arise:\n",
        "\n",
        "1. **Small Chunks** : Needed for accurate meaning representation in embeddings\n",
        "2. **Context Preservation** : Required for maintaining document coherence\n",
        "\n",
        "> How It Works\n",
        "\n",
        "`ParentDocumentRetriever` manages this balance by:\n",
        "\n",
        "1. Splitting documents into small searchable chunks\n",
        "2. Maintaining connections to parent documents via IDs\n",
        "3. Loading multiple files through `TextLoader` objects\n",
        "\n",
        "> Benefits\n",
        "\n",
        "1. **Efficient Search:** Quick identification of relevant content\n",
        "2. **Context Awareness:** Access to broader document context when needed\n",
        "3. **Flexible Structure:** Works with both complete documents and larger chunks as parent documents\n",
        "\n",
        "### Table of Contents\n",
        "\n",
        "- [Overview](#overview)\n",
        "- [Environment Setup](#environment-setup)\n",
        "- [Full Document Retrieval](#full-document-retrieval)\n",
        "- [Adjusting Larger Chunk Sizes](#adjusting-larger-chunk-sizes)\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6c7aba4",
      "metadata": {
        "id": "c6c7aba4"
      },
      "source": [
        "## Environment Setup\n",
        "\n",
        "Set up the environment. You may refer to [Environment Setup](https://wikidocs.net/257836) for more details.\n",
        "\n",
        "**[Note]**\n",
        "- `langchain-opentutorial` is a package that provides a set of easy-to-use environment setup, useful functions and utilities for tutorials.\n",
        "- You can checkout the [`langchain-opentutorial`](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "21943adb",
      "metadata": {
        "id": "21943adb"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install langchain-opentutorial langchain_chroma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "f25ec196",
      "metadata": {
        "id": "f25ec196"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "from langchain_opentutorial import package\n",
        "\n",
        "package.install(\n",
        "    [\n",
        "        \"langsmith\",\n",
        "        \"langchain\",\n",
        "        \"langchain_community\",\n",
        "        \"langchain_openai\",\n",
        "        \"chromadb\",\n",
        "    ],\n",
        "    verbose=False,\n",
        "    upgrade=False,\n",
        ")\n",
        "\n",
        "from langchain.storage import InMemoryStore\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain.retrievers import ParentDocumentRetriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "7f9065ea",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7f9065ea",
        "outputId": "419ff60c-2d09-4937-ab2d-571be3efd273"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment variables have been set successfully.\n"
          ]
        }
      ],
      "source": [
        "# Set environment variables\n",
        "from langchain_opentutorial import set_env\n",
        "\n",
        "set_env(\n",
        "    {\n",
        "        \"OPENAI_API_KEY\": \"\",\n",
        "        \"LANGCHAIN_API_KEY\": \"\",\n",
        "        \"LANGCHAIN_TRACING_V2\": \"true\",\n",
        "        \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
        "        \"LANGCHAIN_PROJECT\": \"Parent-Document-Retriever\",\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "690a9ae0",
      "metadata": {
        "id": "690a9ae0"
      },
      "source": [
        "You can alternatively set API keys such as `OPENAI_API_KEY` in a `.env` file and load them.\n",
        "\n",
        "[Note] This is not necessary if you've already set the required API keys in previous steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4f99b5b6",
      "metadata": {
        "id": "4f99b5b6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load API keys from .env file\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv(override=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "733cc6f6",
      "metadata": {},
      "source": [
        "First, let's load the documents that we'll use as data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "6382b965",
      "metadata": {},
      "outputs": [],
      "source": [
        "loaders = [\n",
        "    # load file (It could be multiple files)\n",
        "    TextLoader(\"./data/appendix-keywords.txt\"),\n",
        "]\n",
        "# If your os is window, execute the following line \n",
        "# loader = TextLoader(\"./data/appendix-keywords.txt\", encoding=\"utf-8\")\n",
        "\n",
        "docs = []\n",
        "for loader in loaders:\n",
        "    # Load the document using the loader and add it to the docs list.\n",
        "    docs.extend(loader.load())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "8a7d148d",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'source': './data/appendix-keywords.txt'}, page_content='Semantic Search\\n\\nDefinition: Semantic search refers to a search method that understands the meaning behind user queries, going beyond simple keyword matching to return relevant results.  \\nExample: When a user searches for \"solar system planets,\" the search returns information about related planets like Jupiter and Mars.  \\nRelated Keywords: Natural Language Processing, Search Algorithms, Data Mining  \\n\\n\\nEmbedding\\n\\nDefinition: Embedding is the process of converting textual data, such as words or sentences, into low-dimensional continuous vectors. This enables computers to understand and process text.  \\nExample: The word \"apple\" can be represented as a vector like [0.65, -0.23, 0.17].  \\nRelated Keywords: Natural Language Processing, Vectorization, Deep Learning  \\n\\n\\nToken\\n\\nDefinition: A token refers to smaller units of text obtained by breaking it into parts, such as words, sentences, or phrases.  \\nExample: The sentence \"I go to school\" can be split into tokens like \"I,\" \"go,\" \"to,\" and \"school.\"  \\nRelated Keywords: Tokenization, Natural Language Processing, Syntax Analysis  \\n\\n\\nTokenizer\\n\\nDefinition: A tokenizer is a tool used to divide text data into tokens, often as part of preprocessing in natural language processing.  \\nExample: The sentence \"I love programming.\" is split into [\"I\", \"love\", \"programming\", \".\"].  \\nRelated Keywords: Tokenization, Natural Language Processing, Syntax Analysis  \\n\\n\\nVectorStore\\n\\nDefinition: A vector store is a system that stores data transformed into vector formats. It is used in tasks like search, classification, and data analysis.  \\nExample: Word embedding vectors are stored in a database for quick access.  \\nRelated Keywords: Embedding, Database, Vectorization  \\n\\n\\nSQL\\n\\nDefinition: SQL (Structured Query Language) is a programming language used to manage data in databases. It supports tasks like querying, modifying, inserting, and deleting data.  \\nExample: `SELECT * FROM users WHERE age > 18;` retrieves information about users over 18 years old.  \\nRelated Keywords: Database, Query, Data Management  \\n\\n\\nCSV\\n\\nDefinition: CSV (Comma-Separated Values) is a file format used to store data, where each value is separated by a comma. It is often used for saving and exchanging tabular data.  \\nExample: A CSV file with headers \"Name, Age, Job\" could contain data like \"John, 30, Developer.\"  \\nRelated Keywords: Data Format, File Handling, Data Exchange  \\n\\n\\nJSON\\n\\nDefinition: JSON (JavaScript Object Notation) is a lightweight data interchange format that uses text to represent data objects in a readable manner for both humans and machines.  \\nExample: `{\"Name\": \"John\", \"Age\": 30, \"Job\": \"Developer\"}` is an example of JSON-formatted data.  \\nRelated Keywords: Data Exchange, Web Development, API  \\n\\n\\nTransformer\\n\\nDefinition: A transformer is a type of deep learning model used in natural language processing for tasks like translation, summarization, and text generation. It is based on the attention mechanism.  \\nExample: Google Translate uses transformer models for multilingual translations.  \\nRelated Keywords: Deep Learning, Natural Language Processing, Attention  \\n\\n\\nHuggingFace\\n\\nDefinition: HuggingFace is a library that provides pre-trained models and tools for natural language processing, making it easier for researchers and developers to perform NLP tasks.  \\nExample: Using HuggingFace\\'s Transformers library to perform sentiment analysis or text generation.  \\nRelated Keywords: Natural Language Processing, Deep Learning, Library  \\n\\n\\nDigital Transformation\\n\\nDefinition: Digital transformation refers to the process of leveraging technology to innovate services, culture, and operations within a business, improving business models and competitiveness through digital technologies.  \\nExample: A company adopting cloud computing to revolutionize data storage and processing.  \\nRelated Keywords: Innovation, Technology, Business Model  \\n\\n\\nCrawling\\n\\nDefinition: Crawling is the automated process of visiting web pages to collect data. It is commonly used for search engine optimization and data analysis.  \\nExample: Google’s search engine crawls websites to collect and index content.  \\nRelated Keywords: Data Collection, Web Scraping, Search Engine  \\n\\n\\nWord2Vec\\n\\nDefinition: Word2Vec is a natural language processing technique that maps words to vector spaces, representing semantic relationships between words.  \\nExample: In a Word2Vec model, \"king\" and \"queen\" are represented as vectors close to each other in the vector space.  \\nRelated Keywords: Natural Language Processing, Embedding, Semantic Similarity  \\n\\n\\nLLM (Large Language Model)\\n\\nDefinition: An LLM is a large-scale language model trained on massive text datasets, used for various natural language understanding and generation tasks.  \\nExample: OpenAI\\'s GPT series is a prominent example of LLMs.  \\nRelated Keywords: Natural Language Processing, Deep Learning, Text Generation  \\n\\n\\nFAISS (Facebook AI Similarity Search)\\n\\nDefinition: FAISS is a high-speed similarity search library developed by Facebook, designed for efficiently searching large sets of vectors for similar ones.  \\nExample: Using FAISS to quickly find similar images in a dataset containing millions of image vectors.  \\nRelated Keywords: Vector Search, Machine Learning, Database Optimization  \\n\\n\\nOpen Source\\n\\nDefinition: Open source refers to software where the source code is publicly available for anyone to use, modify, and distribute, promoting collaboration and innovation.  \\nExample: The Linux operating system is a widely known open-source project.  \\nRelated Keywords: Software Development, Community, Technology Collaboration  \\n\\n\\nStructured Data\\nDefinition: Organized data following a predefined format or schema, easily searchable and analyzable in databases and spreadsheets.\\nExample: Customer information table stored in a relational database.\\nRelated Keywords: database, data analysis, data modeling\\n\\n\\nParser\\nDefinition: A tool that analyzes given data (strings, files, etc.) and converts it into a structured format, used in programming language syntax analysis and file data processing.\\nExample: Parsing HTML documents to create DOM structure of web pages.\\nRelated Keywords: syntax analysis, compiler, data processing\\n\\n\\nTF-IDF (Term Frequency-Inverse Document Frequency)\\nDefinition: A statistical measure used to evaluate word importance in documents, considering both word frequency in a document and its rarity across all documents.\\nExample: Words that appear infrequently across many documents have high TF-IDF values.\\nRelated Keywords: natural language processing, information retrieval, data mining\\n\\n\\nDeep Learning\\nDefinition: A machine learning field using artificial neural networks to solve complex problems, focusing on learning high-level data representations.\\nExample: Used in image recognition, speech recognition, and natural language processing.\\nRelated Keywords: artificial neural networks, machine learning, data analysis\\n\\n\\nSchema\\nDefinition: Blueprint defining database or file structure, specifying how data is stored and organized.\\nExample: Relational database table schemas define column names, data types, and key constraints.\\nRelated Keywords: database, data modeling, data management\\n\\n\\nDataFrame\\nDefinition: Table-like data structure with rows and columns, primarily used for data analysis and processing.\\nExample: Pandas library DataFrames can contain various data types and facilitate data manipulation and analysis.\\nRelated Keywords: data analysis, pandas, data processing\\n\\n\\nAttention Mechanism\\nDefinition: A deep learning technique that enables models to focus more on important information, primarily used with sequence data.\\nExample: Translation models use attention to focus on important parts of input sentences for accurate translation.\\nRelated Keywords: deep learning, natural language processing, sequence modeling\\n\\n\\nPandas\\nDefinition: A Python library providing tools for data analysis and manipulation, enabling efficient data analysis tasks.\\nExample: Used to read CSV files, clean data, and perform various analyses.\\nRelated Keywords: data analysis, Python, data processing\\n\\n\\nGPT (Generative Pretrained Transformer)\\nDefinition: A pre-trained generative language model for various text-based tasks, capable of generating natural language based on input text.\\nExample: Chatbots generating detailed responses to user questions using GPT models.\\nRelated Keywords: natural language processing, text generation, deep learning\\n\\n\\nInstructGPT\\nDefinition: An optimized GPT model designed to perform specific tasks based on user instructions, producing more accurate and relevant results.\\nExample: Creating email drafts based on user instructions.\\nRelated Keywords: artificial intelligence, natural language understanding, instruction-based processing\\n\\n\\nKeyword Search\\nDefinition: Process of finding information based on user-input keywords, fundamental to most search engines and database systems.\\nExample: Searching \"coffee shop Seoul\" returns relevant coffee shop listings.\\nRelated Keywords: search engine, data search, information retrieval\\n\\n\\nPage Rank\\nDefinition: Algorithm evaluating webpage importance by analyzing web page link structures, used in search engine result ranking.\\nExample: Google search engine uses PageRank to determine search result rankings.\\nRelated Keywords: search engine optimization, web analytics, link analysis\\n\\n\\nData Mining\\nDefinition: Process of discovering useful information from large datasets using statistics, machine learning, and pattern recognition.\\nExample: Retail stores analyzing customer purchase data to develop sales strategies.\\nRelated Keywords: big data, pattern recognition, predictive analytics\\n\\n\\nMultimodal\\nDefinition: Technology processing multiple data modes (text, images, sound, etc.) to extract or predict more accurate information through interaction between different data formats.\\nExample: Systems analyzing both images and descriptive text for more accurate image classification.\\nRelated Keywords: data fusion, artificial intelligence, deep learning')]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15356c17",
      "metadata": {},
      "source": [
        "## Full Document Retrieval\n",
        "\n",
        "In this mode, we aim to search through complete documents. Therefore, we'll only specify the `child_splitter`.\n",
        "\n",
        "Later, we'll also specify the `parent_splitter` to compare the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "1db5c338",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define Child Splitter with chunk size\n",
        "child_splitter = RecursiveCharacterTextSplitter(chunk_size=200)\n",
        "\n",
        "# Create a Chroma DB collection -- in memory version\n",
        "vectorstore = Chroma(\n",
        "    collection_name=\"full_documents\", embedding_function=OpenAIEmbeddings()\n",
        ")\n",
        "\n",
        "store = InMemoryStore()\n",
        "\n",
        "# Create Retriever\n",
        "retriever = ParentDocumentRetriever(\n",
        "    vectorstore=vectorstore,\n",
        "    docstore=store,\n",
        "    child_splitter=child_splitter,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f4faa20",
      "metadata": {},
      "source": [
        "Documents are added using the `retriever.add_documents(docs, ids=None)` function:\n",
        "* If `ids` is `None`, they will be automatically generated.\n",
        "* Setting `add_to_docstore=False` prevents duplicate document additions. However, `ids` values are required to check for duplicates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ab710019",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add documents to the retriever. 'docs' is a list of documents, and 'ids' is a list of unique document identifiers.\n",
        "retriever.add_documents(docs, ids=None, add_to_docstore=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba46704a",
      "metadata": {},
      "source": [
        "This code should return two keys because we added two documents.\n",
        "\n",
        "- Convert the keys returned by the `store` object's `yield_keys()` method into a list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "2e34a4c3",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['739c2480-1aac-4090-ae21-ceebc416d099']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Return all keys from the store as a list.\n",
        "list(store.yield_keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2c82732",
      "metadata": {},
      "source": [
        "Let's try calling the vector store search function.\n",
        "\n",
        "Since we are storing small chunks, we should see small chunks returned in the search results.\n",
        "\n",
        "Perform similarity search using the `similarity_search` method of the vectorstore object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "26d84a22",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word2Vec\n"
          ]
        }
      ],
      "source": [
        "# Perform similarity search\n",
        "sub_docs = vectorstore.similarity_search(\"Word2Vec\")\n",
        "\n",
        "# Print the page_content property of the first element in the sub_docs list.\n",
        "print(sub_docs[0].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0dbe1682",
      "metadata": {},
      "source": [
        "Now let's search through the entire retriever. In this process, since it **returns the documents** containing the small chunks, relatively larger documents will be returned.\n",
        "\n",
        "Use the `invoke()` method of the `retriever` object to retrieve documents related to the query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "e9c9d979",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Retrieve and fetch documents\n",
        "retrieved_docs = retriever.invoke(\"Word2Vec\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "2c9be3aa",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Document length: 10044\n",
            "\n",
            "=====================\n",
            "\n",
            " old.  \n",
            "Related Keywords: Database, Query, Data Management  \n",
            "\n",
            "\n",
            "CSV\n",
            "\n",
            "Definition: CSV (Comma-Separated Values) is a file format used to store data, where each value is separated by a comma. It is often used for saving and exchanging tabular data.  \n",
            "Example: A CSV file with headers \"Name, Age, Job\" could contain data like \"John, 30, Developer.\"  \n",
            "Related Keywords: Data Format, File Handling, Data Exchange  \n",
            "\n",
            "\n",
            "JSON\n",
            "\n",
            "Definition: JSON (JavaScript Object Notation) is a lightweight data interchange form\n"
          ]
        }
      ],
      "source": [
        "# Print the length of the page content of the retrieved document\n",
        "print(\n",
        "    f\"Document length: {len(retrieved_docs[0].page_content)}\",\n",
        "    end=\"\\n\\n=====================\\n\\n\",\n",
        ")\n",
        "\n",
        "# Print a portion of the document\n",
        "print(retrieved_docs[0].page_content[2000:2500])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25659b35",
      "metadata": {},
      "source": [
        "## Adjusting Larger Chunk Sizes\n",
        "\n",
        "Like the previous results, **the entire document may be too large to search through as is** .\n",
        "\n",
        "In this case, what we actually want to do is first split the raw document into larger chunks, and then split those into smaller chunks.\n",
        "\n",
        "Then we index the small chunks, but search for larger chunks during retrieval (though still not the entire document).\n",
        "\n",
        "- Use `RecursiveCharacterTextSplitter` to create parent and child documents.\n",
        "\n",
        "    - Parent documents have `chunk_size` set to 1000.\n",
        "    - Child documents have `chunk_size` set to 200, creating smaller sizes than the parent documents.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "5a398e04",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Text splitter used to generate parent documents\n",
        "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=1000)\n",
        "\n",
        "# Text splitter used to generate child documents\n",
        "# Should create documents smaller than the parent\n",
        "child_splitter = RecursiveCharacterTextSplitter(chunk_size=200)\n",
        "\n",
        "# Vector store to be used for indexing child chunks\n",
        "vectorstore = Chroma(\n",
        "    collection_name=\"split_parents\", embedding_function=OpenAIEmbeddings()\n",
        ")\n",
        "# Storage layer for parent documents\n",
        "store = InMemoryStore()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e752ae5",
      "metadata": {},
      "source": [
        "This is the code to initialize `ParentDocumentRetriever`:\n",
        "* The `vectorstore` parameter specifies the vector store that stores document vectors.\n",
        "* The `docstore` parameter specifies the document store that stores document data.\n",
        "* The `child_splitter` parameter specifies the document splitter used to split child documents.\n",
        "* The `parent_splitter` parameter specifies the document splitter used to split parent documents.\n",
        "\n",
        "`ParentDocumentRetriever` handles hierarchical document structures, separately splitting and storing parent and child documents. This allows effective use of both parent and child documents during retrieval."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "660df0ae",
      "metadata": {},
      "outputs": [],
      "source": [
        "retriever = ParentDocumentRetriever(\n",
        "    # Specify the vector store\n",
        "    vectorstore=vectorstore,\n",
        "    # Specify the document store\n",
        "    docstore=store,\n",
        "    # Specify the child document splitter\n",
        "    child_splitter=child_splitter,\n",
        "    # Specify the parent document splitter\n",
        "    parent_splitter=parent_splitter,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9aec94e",
      "metadata": {},
      "source": [
        "Add docs to the `retriever` object. This adds new documents to the set of documents that `retriever` can search through."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "c1068426",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add documents to the retriever\n",
        "retriever.add_documents(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c882ce91",
      "metadata": {},
      "source": [
        "Now you can see there are many more documents. These are the larger chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "24f79410",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Generate keys from the store, convert to list, and return the length\n",
        "len(list(store.yield_keys()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "469eb179",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word2Vec\n"
          ]
        }
      ],
      "source": [
        "# Perform similarity search\n",
        "sub_docs = vectorstore.similarity_search(\"Word2Vec\")\n",
        "# Print the page_content property of the first element in the sub_docs list\n",
        "print(sub_docs[0].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc0a62a1",
      "metadata": {},
      "source": [
        "Now let's use the `invoke()` method of the `retriever` object to search for documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "dcddd245",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Crawling\n",
            "\n",
            "Definition: Crawling is the automated process of visiting web pages to collect data. It is commonly used for search engine optimization and data analysis.  \n",
            "Example: Google’s search engine crawls websites to collect and index content.  \n",
            "Related Keywords: Data Collection, Web Scraping, Search Engine  \n",
            "\n",
            "\n",
            "Word2Vec\n",
            "\n",
            "Definition: Word2Vec is a natural language processing technique that maps words to vector spaces, representing semantic relationships between words.  \n",
            "Example: In a Word2Vec model, \"king\" and \"queen\" are represented as vectors close to each other in the vector space.  \n",
            "Related Keywords: Natural Language Processing, Embedding, Semantic Similarity  \n",
            "\n",
            "\n",
            "LLM (Large Language Model)\n"
          ]
        }
      ],
      "source": [
        "# Retrieve and fetch documents\n",
        "retrieved_docs = retriever.invoke(\"Word2Vec\")\n",
        "\n",
        "# Return the length of the page content of the first retrieved document\n",
        "print(retrieved_docs[0].page_content)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "langchain-kr-lwwSZlnu-py3.11",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
